{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_UD_sents import get_trainable_data, read_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_map = {\n",
    "    'ADJ': 0,\n",
    "    'ADP': 1,\n",
    "    'ADV': 2,\n",
    "    'AUX': 3,\n",
    "    'CCONJ': 4,\n",
    "    'DET': 5,\n",
    "    'INTJ': 6,\n",
    "    'NOUN': 7,\n",
    "    'NUM': 8,\n",
    "    'PART': 9,\n",
    "    'PRON': 10,\n",
    "    'PROPN': 11,\n",
    "    'PUNCT': 12,\n",
    "    'SCONJ': 13,\n",
    "    'SYM': 14,\n",
    "    'UNK': 15,\n",
    "    'VERB': 16,\n",
    "    'X': 17\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 files in ../data/UD_scrambled_sents_pickles, loading pickled files...\n"
     ]
    }
   ],
   "source": [
    "# store these as parsed texts...\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "train, test, dev = None, None, None\n",
    "folder = \"../data/UD_scrambled_sents_pickles\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "contents = os.listdir(folder)\n",
    "\n",
    "if not os.path.exists(folder) or len(contents) == 0:\n",
    "    train = get_trainable_data(\"train\", pos_map)\n",
    "    test = get_trainable_data(\"test\", pos_map)\n",
    "    dev = get_trainable_data(\"dev\", pos_map)\n",
    "\n",
    "    print(f\"No files exist in {folder}, dumping pickled files...\")\n",
    "    for name, data in zip([\"train\", \"test\", \"dev\"], [train, test, dev]):\n",
    "        with open(os.path.join(folder, f\"{name}.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "else:\n",
    "    print(f\"Found {len(contents)} files in {folder}, loading pickled files...\")\n",
    "    # load based on names in listdir:\n",
    "    for content in contents:\n",
    "        with open(os.path.join(folder, content), \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            if \"train\" in content:\n",
    "                train = data\n",
    "            elif \"test\" in content:\n",
    "                test = data\n",
    "            elif \"dev\" in content:\n",
    "                dev = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.X, train.y\n",
    "X_test, y_test = test.X, test.y\n",
    "X_dev, y_dev = dev.X, dev.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len: 34\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 34, 32)            576       \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 34, 256)          164864    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 256)              394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                8224      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 567,937\n",
      "Trainable params: 567,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "4927/4927 [==============================] - 316s 64ms/step - loss: 0.1560 - accuracy: 0.9537 - val_loss: 0.1046 - val_accuracy: 0.9674\n",
      "Epoch 2/20\n",
      "4927/4927 [==============================] - 317s 64ms/step - loss: 0.0960 - accuracy: 0.9731 - val_loss: 0.0808 - val_accuracy: 0.9799\n",
      "Epoch 3/20\n",
      "4927/4927 [==============================] - 317s 64ms/step - loss: 0.0758 - accuracy: 0.9827 - val_loss: 0.0687 - val_accuracy: 0.9828\n",
      "Epoch 4/20\n",
      "4927/4927 [==============================] - 318s 64ms/step - loss: 0.0672 - accuracy: 0.9848 - val_loss: 0.0649 - val_accuracy: 0.9845\n",
      "Epoch 5/20\n",
      "4927/4927 [==============================] - 318s 65ms/step - loss: 0.0633 - accuracy: 0.9858 - val_loss: 0.0608 - val_accuracy: 0.9850\n",
      "Epoch 6/20\n",
      "4927/4927 [==============================] - 320s 65ms/step - loss: 0.0604 - accuracy: 0.9867 - val_loss: 0.0601 - val_accuracy: 0.9855\n",
      "Epoch 7/20\n",
      "4927/4927 [==============================] - 309s 63ms/step - loss: 0.0586 - accuracy: 0.9870 - val_loss: 0.0613 - val_accuracy: 0.9858\n",
      "Epoch 8/20\n",
      "4927/4927 [==============================] - 302s 61ms/step - loss: 0.0563 - accuracy: 0.9879 - val_loss: 0.0589 - val_accuracy: 0.9861\n",
      "Epoch 9/20\n",
      " 913/4927 [====>.........................] - ETA: 3:54 - loss: 0.0548 - accuracy: 0.9882"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "MAX_LEN = X_train.shape[1]\n",
    "print(f\"Max len: {MAX_LEN}\")\n",
    "embedding_size = len(pos_map)\n",
    "\n",
    "l2 = tf.keras.regularizers.l2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(embedding_size, 32, input_length=MAX_LEN),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=l2(0.02)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(0.02))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_dev, y_dev),\n",
    "                    epochs=20,\n",
    "                    batch_size=64\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"nb_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94974697"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Frakoblingen skjedde ikke, og raketten begynte å spinne. Den spant i omtrent ett minutt før det kom en eksplosjon som ødela begge deler av raketten.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "def pred_from_pos(pos):\n",
    "    pos = np.pad(pos, (0, MAX_LEN - len(pos)), constant_values=\"UNK\")\n",
    "    pos = np.vectorize(pos_map.get)(pos)\n",
    "    pred = model.predict(np.asarray([pos]), verbose=0)[0][0]\n",
    "    return pred\n",
    "\n",
    "def pred_from_doc(doc):\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    return pred_from_pos(pos)\n",
    "\n",
    "def pred_from_text(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    return pred_from_doc(doc)\n",
    "\n",
    "pred_from_doc(doc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \"\"\"\n",
    "Oppskytningen av raketten «Super Heavy/Starship» feilet. Dette var det første forsøket på å bruke den nye rakett-typen. Raketten skal lande mennesker på månen, og SpaceX har planer om å sende den til Mars.\n",
    "\n",
    "SpaceX sa kort tid etter eksplosjonen at oppskytningen var delvis velykket. Det på grunn av at raketten ikke eksploderte ved start. Selskapet har samlet mye data som vil informere dem om hva som gikk galt.\n",
    "\n",
    "Det første tegnet på mulige problemer var synlig allerede 16 sekunder etter start. Tre av motorene i det første trinnet fyrte ikke. Det kommer fram i vidoen fra oppskytningen. 40 sekunder ut i ferden falt én motor til ut. 20 sekunder senere falt den femte motoren ut.\n",
    "\n",
    "Etter planen skulle bæreraketten Super Heavy skille seg vekk fra romfartøyet Starship omtrent 2 minutter og 40 sekunder etter start.\n",
    "\n",
    "Frakoblingen skjedde ikke, og raketten begynte å spinne. Den spant i omtrent ett minutt før det kom en eksplosjon som ødela begge deler av raketten.\n",
    "\"\"\"\n",
    "\n",
    "texts = \"\"\"\n",
    "Nordmenn spiser for mye kjøtt\n",
    "I kroppen vår har vi et hormon som heter insulin. Dette gjør oss i stand til å omdanne sukkeret vi spiser til energi. Når man har diabetes type 2 virker ikke insulinet som det skal, og man får høyt blodsukker fordi mye av sukkeret blir værende i blodet.\n",
    "\n",
    "Ifølge tall har omtrent 240.000 mennesker i Norge denne livsstilssykdommen. I tillegg regner man med at en del folk lever med sykdommen uten å være klar over det.\n",
    "\n",
    "I tillegg til dårlig kosthold kan overvekt, inaktivitet, røyking og alkohol utløse sykdommen. Arv er også avgjørende. Har du mor eller far med diabetes type 2, har du selv omtrent 40 prosent sjanse for å utvikle sykdommen i løpet av livet.\n",
    "\n",
    "Resultatene fra den nye studien viser at man i Norge i 1990 estimerte at 64,9 prosent av tilfellene av sykdommen skyldtes dårlig kosthold. I 2018 hadde dette tallet steget til 75,1 prosent.\n",
    "\n",
    "Og grunnen til dette? Vi spiser for mye rødt og bearbeidet kjøtt, mener forskerne. I tillegg har vi nordmenn et utilstrekkelig inntak av fullkorn.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nordmenn spiser for mye kjøtt\n",
      "Original: ['NOUN', 'VERB', 'ADV', 'ADJ', 'NOUN']\n",
      "--> Grammatical (55% confidence)\n",
      "\tShuffled 0: ['NOUN', 'ADJ', 'NOUN', 'ADV', 'VERB']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['NOUN', 'NOUN', 'VERB', 'ADV', 'ADJ']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['ADJ', 'VERB', 'NOUN', 'NOUN', 'ADV']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "____________________________________________________________\n",
      "I kroppen vår har vi et hormon som heter insulin.\n",
      "Original: ['ADP', 'NOUN', 'PRON', 'VERB', 'PRON', 'DET', 'NOUN', 'PRON', 'VERB', 'NOUN', 'PUNCT']\n",
      "--> Grammatical (93% confidence)\n",
      "\tShuffled 0: ['ADP', 'PUNCT', 'NOUN', 'PRON', 'VERB', 'PRON', 'NOUN', 'NOUN', 'DET', 'PRON', 'VERB']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['DET', 'NOUN', 'NOUN', 'PRON', 'PUNCT', 'VERB', 'PRON', 'PRON', 'ADP', 'VERB', 'NOUN']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['PUNCT', 'PRON', 'VERB', 'NOUN', 'VERB', 'NOUN', 'ADP', 'NOUN', 'DET', 'PRON', 'PRON']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "Dette gjør oss i stand til å omdanne sukkeret vi spiser til energi.\n",
      "Original: ['PRON', 'VERB', 'PRON', 'SCONJ', 'NOUN', 'SCONJ', 'PART', 'VERB', 'NOUN', 'PRON', 'VERB', 'ADP', 'NOUN', 'PUNCT']\n",
      "--> Grammatical (95% confidence)\n",
      "\tShuffled 0: ['NOUN', 'PART', 'PUNCT', 'VERB', 'NOUN', 'VERB', 'SCONJ', 'PRON', 'PRON', 'VERB', 'ADP', 'PRON', 'NOUN', 'SCONJ']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['PRON', 'PRON', 'PART', 'NOUN', 'SCONJ', 'NOUN', 'PRON', 'PUNCT', 'VERB', 'VERB', 'VERB', 'NOUN', 'ADP', 'SCONJ']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['VERB', 'VERB', 'PART', 'VERB', 'PRON', 'PRON', 'PRON', 'ADP', 'PUNCT', 'SCONJ', 'NOUN', 'NOUN', 'NOUN', 'SCONJ']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "Når man har diabetes type 2 virker ikke insulinet som det skal, og man får høyt blodsukker fordi mye av sukkeret blir værende i blodet.\n",
      "Original: ['SCONJ', 'PRON', 'AUX', 'VERB', 'NOUN', 'NUM', 'VERB', 'PART', 'NOUN', 'PRON', 'PRON', 'VERB', 'PUNCT', 'CCONJ', 'PRON', 'VERB', 'ADJ', 'NOUN', 'SCONJ', 'ADJ', 'ADP', 'NOUN', 'VERB', 'ADJ', 'ADP', 'NOUN', 'PUNCT']\n",
      "--> Grammatical (95% confidence)\n",
      "\tShuffled 0: ['VERB', 'ADP', 'VERB', 'ADJ', 'ADP', 'PRON', 'VERB', 'AUX', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'SCONJ', 'NOUN', 'CCONJ', 'PART', 'ADJ', 'VERB', 'PRON', 'PRON', 'PUNCT', 'NUM', 'PRON', 'SCONJ', 'PUNCT', 'ADJ']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['PART', 'VERB', 'VERB', 'NOUN', 'NOUN', 'PRON', 'CCONJ', 'ADP', 'PUNCT', 'SCONJ', 'PUNCT', 'PRON', 'NOUN', 'SCONJ', 'PRON', 'NOUN', 'ADJ', 'ADJ', 'NUM', 'VERB', 'ADP', 'VERB', 'ADJ', 'AUX', 'VERB', 'PRON', 'NOUN']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['AUX', 'PUNCT', 'PRON', 'PART', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'SCONJ', 'VERB', 'ADJ', 'CCONJ', 'NUM', 'PRON', 'VERB', 'SCONJ', 'ADJ', 'PRON', 'NOUN', 'VERB', 'PUNCT', 'VERB', 'VERB', 'ADP', 'PRON', 'ADP']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "____________________________________________________________\n",
      "Ifølge tall har omtrent 240.000 mennesker i Norge denne livsstilssykdommen.\n",
      "Original: ['ADP', 'NOUN', 'VERB', 'ADV', 'NUM', 'NOUN', 'ADP', 'PROPN', 'DET', 'NOUN', 'PUNCT']\n",
      "--> Grammatical (95% confidence)\n",
      "\tShuffled 0: ['PROPN', 'NOUN', 'VERB', 'NOUN', 'NUM', 'PUNCT', 'ADP', 'ADV', 'ADP', 'NOUN', 'DET']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['PUNCT', 'NOUN', 'ADP', 'NUM', 'ADV', 'ADP', 'NOUN', 'PROPN', 'NOUN', 'VERB', 'DET']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['VERB', 'ADV', 'ADP', 'NUM', 'NOUN', 'PROPN', 'NOUN', 'ADP', 'DET', 'PUNCT', 'NOUN']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "I tillegg regner man med at en del folk lever med sykdommen uten å være klar over det.\n",
      "Original: ['ADP', 'NOUN', 'VERB', 'PRON', 'SCONJ', 'SCONJ', 'DET', 'NOUN', 'NOUN', 'VERB', 'ADP', 'NOUN', 'SCONJ', 'PART', 'AUX', 'ADJ', 'ADP', 'PRON', 'PUNCT']\n",
      "--> Grammatical (95% confidence)\n",
      "\tShuffled 0: ['AUX', 'NOUN', 'PRON', 'VERB', 'NOUN', 'ADP', 'SCONJ', 'PRON', 'PART', 'NOUN', 'ADJ', 'SCONJ', 'PUNCT', 'NOUN', 'VERB', 'ADP', 'SCONJ', 'ADP', 'DET']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['ADJ', 'PRON', 'PUNCT', 'PRON', 'PART', 'ADP', 'SCONJ', 'ADP', 'VERB', 'VERB', 'NOUN', 'NOUN', 'DET', 'NOUN', 'AUX', 'SCONJ', 'NOUN', 'SCONJ', 'ADP']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['SCONJ', 'PART', 'ADP', 'ADP', 'NOUN', 'NOUN', 'VERB', 'ADJ', 'ADP', 'PRON', 'PRON', 'SCONJ', 'PUNCT', 'AUX', 'NOUN', 'VERB', 'NOUN', 'DET', 'SCONJ']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "____________________________________________________________\n",
      "I tillegg til dårlig kosthold kan overvekt, inaktivitet, røyking og alkohol utløse sykdommen.\n",
      "Original: ['ADP', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'AUX', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'CCONJ', 'NOUN', 'VERB', 'NOUN', 'PUNCT']\n",
      "--> Grammatical (95% confidence)\n",
      "\tShuffled 0: ['NOUN', 'NOUN', 'NOUN', 'CCONJ', 'PUNCT', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'AUX', 'PUNCT', 'PUNCT', 'VERB', 'ADP', 'ADJ', 'NOUN']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['ADJ', 'NOUN', 'NOUN', 'VERB', 'CCONJ', 'NOUN', 'NOUN', 'PUNCT', 'ADP', 'NOUN', 'NOUN', 'AUX', 'NOUN', 'PUNCT', 'PUNCT', 'ADP']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['PUNCT', 'ADP', 'ADP', 'AUX', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'ADJ', 'PUNCT', 'NOUN', 'NOUN', 'VERB', 'CCONJ', 'NOUN', 'NOUN']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "Arv er også avgjørende.\n",
      "Original: ['NOUN', 'AUX', 'ADV', 'ADJ', 'PUNCT']\n",
      "--> Grammatical (87% confidence)\n",
      "\tShuffled 0: ['ADV', 'NOUN', 'AUX', 'PUNCT', 'ADJ']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['ADV', 'ADJ', 'NOUN', 'AUX', 'PUNCT']\n",
      "\t--> Ungrammatical (99% confidence)\n",
      "\tShuffled 2: ['NOUN', 'AUX', 'PUNCT', 'ADJ', 'ADV']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "Har du mor eller far med diabetes type 2, har du selv omtrent 40 prosent sjanse for å utvikle sykdommen i løpet av livet.\n",
      "Original: ['VERB', 'PRON', 'NOUN', 'CCONJ', 'NOUN', 'ADP', 'PROPN', 'NOUN', 'NUM', 'PUNCT', 'VERB', 'PRON', 'DET', 'ADV', 'NUM', 'NOUN', 'NOUN', 'SCONJ', 'PART', 'VERB', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'PUNCT']\n",
      "--> Grammatical (95% confidence)\n",
      "\tShuffled 0: ['NOUN', 'NOUN', 'NUM', 'ADP', 'ADV', 'NOUN', 'PRON', 'NOUN', 'NUM', 'NOUN', 'NOUN', 'VERB', 'PUNCT', 'PRON', 'SCONJ', 'NOUN', 'NOUN', 'PART', 'CCONJ', 'PROPN', 'VERB', 'PUNCT', 'VERB', 'ADP', 'DET', 'ADP']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['NOUN', 'NOUN', 'ADV', 'PART', 'ADP', 'VERB', 'NOUN', 'CCONJ', 'PROPN', 'NOUN', 'ADP', 'NOUN', 'VERB', 'NOUN', 'VERB', 'NUM', 'ADP', 'SCONJ', 'NOUN', 'PUNCT', 'PUNCT', 'NOUN', 'PRON', 'NUM', 'PRON', 'DET']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['ADP', 'ADV', 'NOUN', 'PRON', 'NOUN', 'NOUN', 'ADP', 'CCONJ', 'VERB', 'PROPN', 'PART', 'VERB', 'NOUN', 'DET', 'PRON', 'NOUN', 'PUNCT', 'PUNCT', 'SCONJ', 'NUM', 'NOUN', 'NUM', 'NOUN', 'ADP', 'VERB', 'NOUN']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "____________________________________________________________\n",
      "Resultatene fra den nye studien viser at man i Norge i 1990 estimerte at 64,9 prosent av tilfellene av sykdommen skyldtes dårlig kosthold.\n",
      "Original: ['NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'SCONJ', 'PRON', 'ADP', 'PROPN', 'ADP', 'NUM', 'VERB', 'SCONJ', 'NUM', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'PUNCT']\n",
      "--> Grammatical (95% confidence)\n",
      "\tShuffled 0: ['ADP', 'SCONJ', 'PROPN', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'ADP', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'DET', 'NUM', 'ADP', 'SCONJ', 'PUNCT', 'PRON', 'NUM', 'NOUN', 'ADP', 'NOUN', 'VERB']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['ADJ', 'VERB', 'PROPN', 'NOUN', 'NUM', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'DET', 'VERB', 'PUNCT', 'VERB', 'ADP', 'NUM', 'ADP', 'NOUN', 'ADP', 'PRON', 'SCONJ', 'SCONJ', 'ADP', 'NOUN']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['SCONJ', 'NOUN', 'ADJ', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'VERB', 'ADP', 'NOUN', 'NUM', 'PROPN', 'PRON', 'NOUN', 'ADP', 'NOUN', 'VERB', 'DET', 'NUM', 'VERB', 'ADP', 'PUNCT', 'SCONJ', 'ADP']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "I 2018 hadde dette tallet steget til 75,1 prosent.\n",
      "Original: ['ADP', 'NUM', 'AUX', 'DET', 'NOUN', 'VERB', 'ADP', 'NUM', 'NOUN', 'PUNCT']\n",
      "--> Grammatical (95% confidence)\n",
      "\tShuffled 0: ['ADP', 'VERB', 'NUM', 'ADP', 'DET', 'NOUN', 'NOUN', 'AUX', 'NUM', 'PUNCT']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['ADP', 'ADP', 'NOUN', 'AUX', 'NUM', 'VERB', 'PUNCT', 'NOUN', 'DET', 'NUM']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['NUM', 'VERB', 'ADP', 'NOUN', 'AUX', 'ADP', 'NUM', 'PUNCT', 'DET', 'NOUN']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "____________________________________________________________\n",
      "Og grunnen til dette?\n",
      "Original: ['CCONJ', 'NOUN', 'ADP', 'PRON', 'PUNCT']\n",
      "--> Grammatical (81% confidence)\n",
      "\tShuffled 0: ['ADP', 'NOUN', 'PUNCT', 'CCONJ', 'PRON']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['PUNCT', 'ADP', 'PRON', 'CCONJ', 'NOUN']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['PRON', 'ADP', 'NOUN', 'PUNCT', 'CCONJ']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "Vi spiser for mye rødt og bearbeidet kjøtt, mener forskerne.\n",
      "Original: ['PRON', 'VERB', 'ADV', 'ADJ', 'ADJ', 'CCONJ', 'ADJ', 'NOUN', 'PUNCT', 'VERB', 'NOUN', 'PUNCT']\n",
      "--> Grammatical (95% confidence)\n",
      "\tShuffled 0: ['CCONJ', 'PUNCT', 'NOUN', 'ADV', 'ADJ', 'VERB', 'VERB', 'NOUN', 'PRON', 'ADJ', 'ADJ', 'PUNCT']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['ADJ', 'VERB', 'NOUN', 'PUNCT', 'CCONJ', 'ADJ', 'ADV', 'PUNCT', 'VERB', 'PRON', 'NOUN', 'ADJ']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['ADJ', 'ADV', 'NOUN', 'VERB', 'CCONJ', 'ADJ', 'PRON', 'PUNCT', 'NOUN', 'PUNCT', 'ADJ', 'VERB']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "I tillegg har vi nordmenn et utilstrekkelig inntak av fullkorn.\n",
      "Original: ['ADP', 'NOUN', 'VERB', 'PRON', 'NOUN', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'PUNCT']\n",
      "--> Grammatical (95% confidence)\n",
      "\tShuffled 0: ['NOUN', 'ADP', 'VERB', 'ADJ', 'PRON', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'DET']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 1: ['NOUN', 'NOUN', 'VERB', 'NOUN', 'PUNCT', 'ADP', 'ADP', 'DET', 'PRON', 'ADJ', 'NOUN']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "\tShuffled 2: ['NOUN', 'NOUN', 'ADJ', 'ADP', 'NOUN', 'DET', 'PUNCT', 'NOUN', 'ADP', 'VERB', 'PRON']\n",
      "\t--> Ungrammatical (100% confidence)\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_pred(pred, threshold=0.08):\n",
    "    # badly worded sentences will typically have a score of 0.001 or similar.\n",
    "    res = \"Grammatical\" if pred > threshold else \"Ungrammatical\"\n",
    "    confidence = pred * 100 if pred > threshold else (1 - pred) * 100\n",
    "    return f\"{res} ({confidence:.0f}% confidence)\"\n",
    "\n",
    "def shuffle_spacy_doc(doc):\n",
    "    random_seed = 42\n",
    "    # shuffle on the text and pos tags equally\n",
    "    texts = [token.text for token in doc]\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    random.Random(random_seed).shuffle(texts)\n",
    "    random.Random(random_seed).shuffle(pos)\n",
    "    return texts, pos\n",
    "\n",
    "for text in [t.strip() for t in texts.split(\"\\n\") if len(t) > 0]:\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        pos = [token.pos_ for token in sent]\n",
    "        # create 3 shuffled examples:\n",
    "        print(sent)\n",
    "        print(f\"Original: {pos}\\n--> {parse_pred(pred_from_pos(pos))}\")\n",
    "        for i in range(3):\n",
    "            shuffled = pos.copy()\n",
    "            random.shuffle(shuffled)\n",
    "            print(f\"\\tShuffled {i}: {shuffled}\")\n",
    "            print(f\"\\t--> {parse_pred(pred_from_pos(shuffled))}\")\n",
    "    print(\"__\"*30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tf `model` to disk\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# to file name:\n",
    "filename = f\"models/is_grammatical_{timestamp}.h5\"\n",
    "model.save(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
