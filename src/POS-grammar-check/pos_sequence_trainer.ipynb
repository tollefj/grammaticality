{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_UD_sents import get_trainable_data, read_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_map = {\n",
    "    'ADJ': 0,\n",
    "    'ADP': 1,\n",
    "    'ADV': 2,\n",
    "    'AUX': 3,\n",
    "    'CCONJ': 4,\n",
    "    'DET': 5,\n",
    "    'INTJ': 6,\n",
    "    'NOUN': 7,\n",
    "    'NUM': 8,\n",
    "    'PART': 9,\n",
    "    'PRON': 10,\n",
    "    'PROPN': 11,\n",
    "    'PUNCT': 12,\n",
    "    'SCONJ': 13,\n",
    "    'SYM': 14,\n",
    "    'UNK': 15,\n",
    "    'VERB': 16,\n",
    "    'X': 17\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 files in ../data/UD_scrambled_sents_pickles, loading pickled files...\n"
     ]
    }
   ],
   "source": [
    "# store these as parsed texts...\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "train, test, dev = None, None, None\n",
    "folder = \"../data/UD_scrambled_sents_pickles\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "contents = os.listdir(folder)\n",
    "\n",
    "if not os.path.exists(folder) or len(contents) == 0:\n",
    "    train = get_trainable_data(\"train\", pos_map)\n",
    "    test = get_trainable_data(\"test\", pos_map)\n",
    "    dev = get_trainable_data(\"dev\", pos_map)\n",
    "\n",
    "    print(f\"No files exist in {folder}, dumping pickled files...\")\n",
    "    for name, data in zip([\"train\", \"test\", \"dev\"], [train, test, dev]):\n",
    "        with open(os.path.join(folder, f\"{name}.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "else:\n",
    "    print(f\"Found {len(contents)} files in {folder}, loading pickled files...\")\n",
    "    # load based on names in listdir:\n",
    "    for content in contents:\n",
    "        with open(os.path.join(folder, content), \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            if \"train\" in content:\n",
    "                train = data\n",
    "            elif \"test\" in content:\n",
    "                test = data\n",
    "            elif \"dev\" in content:\n",
    "                dev = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.X, train.y\n",
    "X_test, y_test = test.X, test.y\n",
    "X_dev, y_dev = dev.X, dev.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len: 34\n",
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 34, 32)            576       \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 34, 256)          164864    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                8224      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 567,937\n",
      "Trainable params: 567,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 14:52:28.881348: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m---> 22\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train,\n\u001b[1;32m     23\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(X_dev, y_dev),\n\u001b[1;32m     24\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m\n\u001b[1;32m     26\u001b[0m                     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:959\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[39m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    960\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    962\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_function_spec  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    963\u001b[0m       \u001b[39m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    964\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:142\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m--> 142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:396\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m   args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[1;32m    394\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[0;32m--> 396\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[1;32m    397\u001b[0m     args, kwargs, func_graph)\n\u001b[1;32m    399\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_function_captures  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:300\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[1;32m    299\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 300\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    301\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    303\u001b[0m         args,\n\u001b[1;32m    304\u001b[0m         kwargs,\n\u001b[1;32m    305\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    306\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    307\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m    308\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m    309\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    310\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[1;32m    311\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    313\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    314\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    319\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1113\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1111\u001b[0m   deps_control_manager \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mNullContextmanager()\n\u001b[0;32m-> 1113\u001b[0m \u001b[39mwith\u001b[39;00m func_graph\u001b[39m.\u001b[39mas_default(), deps_control_manager \u001b[39mas\u001b[39;00m deps_ctx:\n\u001b[1;32m   1114\u001b[0m   current_scope \u001b[39m=\u001b[39m variable_scope\u001b[39m.\u001b[39mget_variable_scope()\n\u001b[1;32m   1115\u001b[0m   default_use_resource \u001b[39m=\u001b[39m current_scope\u001b[39m.\u001b[39muse_resource\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/auto_control_deps.py:514\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m do_record:\n\u001b[1;32m    512\u001b[0m       first_use_for_res[input_id] \u001b[39m=\u001b[39m [op]\n\u001b[0;32m--> 514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord_initial_resource_uses \u001b[39mand\u001b[39;00m op_is_stateful(op):\n\u001b[1;32m    515\u001b[0m   \u001b[39mif\u001b[39;00m resource_inputs:\n\u001b[1;32m    516\u001b[0m     resources_by_op[op] \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(resource_inputs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "MAX_LEN = X_train.shape[1]\n",
    "print(f\"Max len: {MAX_LEN}\")\n",
    "embedding_size = len(pos_map)\n",
    "\n",
    "l2 = tf.keras.regularizers.l2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(embedding_size, 32, input_length=MAX_LEN),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=l2(0.02)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(0.02))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_dev, y_dev),\n",
    "                    epochs=20,\n",
    "                    batch_size=64\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"nb_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m s \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFrakoblingen skjedde ikke, og raketten begynte å spinne. Den spant i omtrent ett minutt før det kom en eksplosjon som ødela begge deler av raketten.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m doc \u001b[39m=\u001b[39m nlp(s)\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpred_from_pos\u001b[39m(pos):\n\u001b[1;32m      5\u001b[0m     pos \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpad(pos, (\u001b[39m0\u001b[39m, MAX_LEN \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(pos)), constant_values\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUNK\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "s = \"Frakoblingen skjedde ikke, og raketten begynte å spinne. Den spant i omtrent ett minutt før det kom en eksplosjon som ødela begge deler av raketten.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "def pred_from_pos(pos):\n",
    "    pos = np.pad(pos, (0, MAX_LEN - len(pos)), constant_values=\"UNK\")\n",
    "    pos = np.vectorize(pos_map.get)(pos)\n",
    "    pred = model.predict(np.asarray([pos]), verbose=0)[0][0]\n",
    "    return pred\n",
    "\n",
    "def pred_from_doc(doc):\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    return pred_from_pos(pos)\n",
    "\n",
    "def pred_from_text(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    return pred_from_doc(doc)\n",
    "\n",
    "pred_from_doc(doc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \"\"\"\n",
    "Oppskytningen av raketten «Super Heavy/Starship» feilet. Dette var det første forsøket på å bruke den nye rakett-typen. Raketten skal lande mennesker på månen, og SpaceX har planer om å sende den til Mars.\n",
    "\n",
    "SpaceX sa kort tid etter eksplosjonen at oppskytningen var delvis velykket. Det på grunn av at raketten ikke eksploderte ved start. Selskapet har samlet mye data som vil informere dem om hva som gikk galt.\n",
    "\n",
    "Det første tegnet på mulige problemer var synlig allerede 16 sekunder etter start. Tre av motorene i det første trinnet fyrte ikke. Det kommer fram i vidoen fra oppskytningen. 40 sekunder ut i ferden falt én motor til ut. 20 sekunder senere falt den femte motoren ut.\n",
    "\n",
    "Etter planen skulle bæreraketten Super Heavy skille seg vekk fra romfartøyet Starship omtrent 2 minutter og 40 sekunder etter start.\n",
    "\n",
    "Frakoblingen skjedde ikke, og raketten begynte å spinne. Den spant i omtrent ett minutt før det kom en eksplosjon som ødela begge deler av raketten.\n",
    "\"\"\"\n",
    "\n",
    "texts = \"\"\"\n",
    "Nordmenn spiser for mye kjøtt\n",
    "I kroppen vår har vi et hormon som heter insulin. Dette gjør oss i stand til å omdanne sukkeret vi spiser til energi. Når man har diabetes type 2 virker ikke insulinet som det skal, og man får høyt blodsukker fordi mye av sukkeret blir værende i blodet.\n",
    "\n",
    "Ifølge tall har omtrent 240.000 mennesker i Norge denne livsstilssykdommen. I tillegg regner man med at en del folk lever med sykdommen uten å være klar over det.\n",
    "\n",
    "I tillegg til dårlig kosthold kan overvekt, inaktivitet, røyking og alkohol utløse sykdommen. Arv er også avgjørende. Har du mor eller far med diabetes type 2, har du selv omtrent 40 prosent sjanse for å utvikle sykdommen i løpet av livet.\n",
    "\n",
    "Resultatene fra den nye studien viser at man i Norge i 1990 estimerte at 64,9 prosent av tilfellene av sykdommen skyldtes dårlig kosthold. I 2018 hadde dette tallet steget til 75,1 prosent.\n",
    "\n",
    "Og grunnen til dette? Vi spiser for mye rødt og bearbeidet kjøtt, mener forskerne. I tillegg har vi nordmenn et utilstrekkelig inntak av fullkorn.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     random\u001b[39m.\u001b[39mRandom(random_seed)\u001b[39m.\u001b[39mshuffle(pos)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m texts, pos\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m [t\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m texts\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(t) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]:\n\u001b[1;32m     17\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[1;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_pred(pred, threshold=0.08):\n",
    "    # badly worded sentences will typically have a score of 0.001 or similar.\n",
    "    res = \"Grammatical\" if pred > threshold else \"Ungrammatical\"\n",
    "    confidence = pred * 100 if pred > threshold else (1 - pred) * 100\n",
    "    return f\"{res} ({confidence:.0f}% confidence)\"\n",
    "\n",
    "def shuffle_spacy_doc(doc):\n",
    "    random_seed = 42\n",
    "    # shuffle on the text and pos tags equally\n",
    "    texts = [token.text for token in doc]\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    random.Random(random_seed).shuffle(texts)\n",
    "    random.Random(random_seed).shuffle(pos)\n",
    "    return texts, pos\n",
    "\n",
    "for text in [t.strip() for t in texts.split(\"\\n\") if len(t) > 0]:\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        pos = [token.pos_ for token in sent]\n",
    "        # create 3 shuffled examples:\n",
    "        print(sent)\n",
    "        print(f\"Original: {pos}\\n--> {parse_pred(pred_from_pos(pos))}\")\n",
    "        for i in range(3):\n",
    "            shuffled = pos.copy()\n",
    "            random.shuffle(shuffled)\n",
    "            print(f\"\\tShuffled {i}: {shuffled}\")\n",
    "            print(f\"\\t--> {parse_pred(pred_from_pos(shuffled))}\")\n",
    "    print(\"__\"*30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)[\u001b[39m1\u001b[39m]\n\u001b[1;32m      2\u001b[0m accuracy\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# save tf `model` to disk\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime\n\u001b[1;32m      4\u001b[0m timestamp \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# save tf `model` to disk\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "acc_rounded = int(round(accuracy*100, 0))\n",
    "# to file name:\n",
    "filename = f\"models/is_gram_{timestamp}_{acc_rounded}acc.h5\"\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
